# SuperDog v0.6 ç­–ç•¥å¯¦é©—å®¤æŠ€è¡“è¦æ ¼  
**Strategy Laboratory System Technical Specification**

---

## ğŸ§ª ç³»çµ±æ¦‚è¿°

ç­–ç•¥å¯¦é©—å®¤æ˜¯SuperDog v0.6çš„æ ¸å¿ƒçµ„ä»¶ï¼Œæä¾›å¤§è¦æ¨¡ç­–ç•¥å¯¦é©—ã€åƒæ•¸å„ªåŒ–å’Œçµæœåˆ†æåŠŸèƒ½ã€‚æ”¯æ´æ‰¹é‡å›æ¸¬ã€åƒæ•¸ç¶²æ ¼æœç´¢ã€å¤šæŒ‡æ¨™è©•ä¼°å’Œæ™ºèƒ½çµæœç¯©é¸ã€‚

### æ ¸å¿ƒåƒ¹å€¼
- **æ•ˆç‡æå‡**: å–®æ¬¡é…ç½®åŸ·è¡Œæ•¸ç™¾å€‹å›æ¸¬
- **åƒæ•¸å„ªåŒ–**: è‡ªå‹•æ‰¾å‡ºæœ€ä½³åƒæ•¸çµ„åˆ
- **çµæœåˆ†æ**: å¤šç¶­åº¦æŒ‡æ¨™æ¯”è¼ƒå’Œæ’åº
- **å¤±æ•—å®¹éŒ¯**: å–®å€‹å¤±æ•—ä¸å½±éŸ¿æ•´é«”å¯¦é©—

---

## ğŸ—ï¸ æ¶æ§‹è¨­è¨ˆ

### æ ¸å¿ƒçµ„ä»¶
```
execution_engine/
â”œâ”€â”€ experiments.py              # å¯¦é©—é…ç½®èˆ‡ç®¡ç†
â”œâ”€â”€ experiment_runner.py        # æ‰¹é‡åŸ·è¡Œå¼•æ“
â”œâ”€â”€ parameter_optimizer.py      # åƒæ•¸å„ªåŒ–å™¨
â””â”€â”€ result_analyzer.py          # çµæœåˆ†æå™¨

reports/
â”œâ”€â”€ experiment_store.py         # å¯¦é©—çµæœå­˜å„²
â”œâ”€â”€ experiment_reporter.py      # å ±å‘Šç”Ÿæˆå™¨  
â””â”€â”€ performance_metrics.py      # ç¸¾æ•ˆæŒ‡æ¨™è¨ˆç®—
```

### æ•¸æ“šæµæ¶æ§‹
```
ExperimentConfig â†’ ParameterExpansion â†’ BatchExecution â†’ ResultStorage â†’ Analysis
     â†“                    â†“                  â†“             â†“           â†“
   é…ç½®è§£æ          åƒæ•¸çµ„åˆå±•é–‹        æ‰¹é‡å›æ¸¬åŸ·è¡Œ      çµæœå­˜å„²     åˆ†æå ±å‘Š
```

---

## ğŸ“‹ æ ¸å¿ƒæ•¸æ“šçµæ§‹

### å¯¦é©—é…ç½®
```python
@dataclass
class ExperimentConfig:
    """å¯¦é©—é…ç½®å®Œæ•´å®šç¾©"""
    
    # åŸºæœ¬ä¿¡æ¯
    name: str                           # å¯¦é©—åç¨±
    description: str = ""               # å¯¦é©—æè¿°
    created_by: str = "system"          # å‰µå»ºè€…
    created_at: datetime = field(default_factory=datetime.now)
    
    # ç­–ç•¥è¨­ç½®
    strategy: str                       # ç­–ç•¥åç¨±
    strategy_params_base: Dict = field(default_factory=dict)  # åŸºç¤åƒæ•¸
    
    # å¹£ç¨®è¨­ç½®
    symbol_source: str = "explicit"     # "explicit" | "universe" 
    symbols: List[str] = field(default_factory=list)          # æ˜ç¢ºæŒ‡å®š
    universe_type: str = None           # "large_cap" | "mid_cap" | "small_cap"
    universe_top_n: int = 100           # å¾å®‡å®™å–å‰Nå€‹
    universe_filters: Dict = field(default_factory=dict)      # é¡å¤–ç¯©é¸æ¢ä»¶
    
    # æ™‚é–“è¨­ç½®
    timeframe: str = "1h"               # æ™‚é–“é€±æœŸ
    start_date: str = None              # å›æ¸¬é–‹å§‹æ—¥æœŸ
    end_date: str = None                # å›æ¸¬çµæŸæ—¥æœŸ
    lookback_days: int = 365            # å›æ¸¬å¤©æ•¸ï¼ˆå¦‚æœæœªæŒ‡å®šæ—¥æœŸï¼‰
    
    # åƒæ•¸å„ªåŒ–è¨­ç½®
    param_grid: Dict[str, List] = field(default_factory=dict)  # åƒæ•¸ç¶²æ ¼
    optimization_metric: str = "sharpe_ratio"                  # å„ªåŒ–ç›®æ¨™
    max_combinations: int = 1000        # æœ€å¤§åƒæ•¸çµ„åˆæ•¸
    
    # åŸ·è¡Œè¨­ç½®
    parallel_workers: int = 4           # ä¸¦è¡ŒåŸ·è¡Œæ•¸é‡
    timeout_seconds: int = 300          # å–®æ¬¡å›æ¸¬è¶…æ™‚
    fail_fast: bool = False             # æ˜¯å¦é‡éŒ¯ç«‹å³åœæ­¢
    
    # è¼¸å‡ºè¨­ç½®
    output_metrics: List[str] = field(default_factory=lambda: [
        'total_return', 'sharpe_ratio', 'max_drawdown', 
        'win_rate', 'profit_factor', 'total_trades'
    ])
    save_detailed_results: bool = False  # æ˜¯å¦ä¿å­˜è©³ç´°äº¤æ˜“è¨˜éŒ„

@dataclass
class ParameterCombination:
    """åƒæ•¸çµ„åˆ"""
    
    combination_id: str                 # çµ„åˆå”¯ä¸€ID
    base_params: Dict                   # åŸºç¤åƒæ•¸  
    variable_params: Dict               # è®Šé‡åƒæ•¸
    combined_params: Dict               # åˆä½µå¾Œåƒæ•¸
    priority: int = 0                   # åŸ·è¡Œå„ªå…ˆç´š

@dataclass  
class ExperimentTask:
    """å–®å€‹å¯¦é©—ä»»å‹™"""
    
    task_id: str                        # ä»»å‹™ID
    experiment_name: str                # å¯¦é©—åç¨±
    symbol: str                         # äº¤æ˜“å°
    param_combination: ParameterCombination  # åƒæ•¸çµ„åˆ
    config: ExperimentConfig            # å¯¦é©—é…ç½®
    status: str = "pending"             # pending | running | completed | failed
    start_time: datetime = None         # é–‹å§‹æ™‚é–“
    end_time: datetime = None           # çµæŸæ™‚é–“
    error_message: str = None           # éŒ¯èª¤ä¿¡æ¯

@dataclass
class ExperimentResult:
    """å¯¦é©—çµæœ"""
    
    # ä»»å‹™ä¿¡æ¯
    task_id: str
    experiment_name: str
    symbol: str
    timeframe: str
    start_date: str  
    end_date: str
    
    # åƒæ•¸ä¿¡æ¯
    strategy: str
    params_json: str                    # JSONæ ¼å¼çš„åƒæ•¸
    param_combination_id: str
    
    # ç¸¾æ•ˆæŒ‡æ¨™
    total_return: float
    annualized_return: float
    volatility: float
    sharpe_ratio: float
    max_drawdown: float
    calmar_ratio: float
    
    # äº¤æ˜“çµ±è¨ˆ
    total_trades: int
    win_rate: float
    profit_factor: float
    avg_win: float
    avg_loss: float
    max_consecutive_wins: int
    max_consecutive_losses: int
    
    # é¢¨éšªæŒ‡æ¨™
    var_95: float                       # 95% VaR
    expected_shortfall: float           # æœŸæœ›çŸ­ç¼º
    downside_deviation: float           # ä¸‹è¡Œåå·®
    
    # åŸ·è¡Œä¿¡æ¯
    execution_time_seconds: float
    data_points: int
    data_snapshot_id: str               # æ•¸æ“šå¿«ç…§ID
    created_at: datetime
    
    # å¯é¸è©³ç´°çµæœ
    detailed_trades: List[Dict] = None  # è©³ç´°äº¤æ˜“è¨˜éŒ„
    daily_returns: List[float] = None   # æ—¥å›å ±ç‡åºåˆ—
```

---

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½æ¨¡çµ„

### 1. å¯¦é©—ç®¡ç†å™¨
```python
class ExperimentManager:
    """å¯¦é©—ç®¡ç†æ ¸å¿ƒé¡"""
    
    def __init__(self, data_dir: str = "data/experiments"):
        self.data_dir = Path(data_dir)
        self.universe_manager = UniverseManager()
        self.result_store = ExperimentResultStore()
    
    def create_experiment(self, config: ExperimentConfig) -> str:
        """å‰µå»ºæ–°å¯¦é©—"""
        
        # é©—è­‰é…ç½®
        self._validate_config(config)
        
        # ç”Ÿæˆå¯¦é©—ID
        experiment_id = self._generate_experiment_id(config)
        
        # ä¿å­˜é…ç½®
        config_path = self.data_dir / "configs" / f"{experiment_id}.yml" 
        self._save_config(config, config_path)
        
        return experiment_id
    
    def load_experiment(self, experiment_id: str) -> ExperimentConfig:
        """è¼‰å…¥å¯¦é©—é…ç½®"""
        config_path = self.data_dir / "configs" / f"{experiment_id}.yml"
        return self._load_config(config_path)
    
    def list_experiments(self, status: str = None) -> List[Dict]:
        """åˆ—å‡ºå¯¦é©—"""
        experiments = []
        
        for config_file in (self.data_dir / "configs").glob("*.yml"):
            config = self._load_config(config_file)
            experiment_info = {
                'id': config_file.stem,
                'name': config.name,
                'strategy': config.strategy,
                'created_at': config.created_at,
                'status': self._get_experiment_status(config_file.stem)
            }
            experiments.append(experiment_info)
        
        if status:
            experiments = [e for e in experiments if e['status'] == status]
            
        return sorted(experiments, key=lambda x: x['created_at'], reverse=True)
    
    def _validate_config(self, config: ExperimentConfig) -> None:
        """é©—è­‰å¯¦é©—é…ç½®"""
        
        # é©—è­‰ç­–ç•¥å­˜åœ¨
        available_strategies = list_strategies()
        if config.strategy not in available_strategies:
            raise ValueError(f"ç­–ç•¥ {config.strategy} ä¸å­˜åœ¨")
        
        # é©—è­‰å¹£ç¨®è¨­ç½®
        if config.symbol_source == "explicit" and not config.symbols:
            raise ValueError("æ˜ç¢ºæ¨¡å¼å¿…é ˆæä¾›symbolsåˆ—è¡¨")
            
        if config.symbol_source == "universe" and not config.universe_type:
            raise ValueError("å®‡å®™æ¨¡å¼å¿…é ˆæŒ‡å®šuniverse_type")
        
        # é©—è­‰åƒæ•¸ç¶²æ ¼
        if not config.param_grid:
            raise ValueError("å¿…é ˆæä¾›param_gridåƒæ•¸ç¶²æ ¼")
        
        # è¨ˆç®—åƒæ•¸çµ„åˆç¸½æ•¸
        total_combinations = 1
        for param_values in config.param_grid.values():
            total_combinations *= len(param_values)
            
        if total_combinations > config.max_combinations:
            raise ValueError(f"åƒæ•¸çµ„åˆæ•¸é‡ {total_combinations} è¶…éé™åˆ¶ {config.max_combinations}")
```

### 2. åƒæ•¸å±•é–‹å™¨
```python
class ParameterExpander:
    """åƒæ•¸çµ„åˆå±•é–‹å™¨"""
    
    def __init__(self, config: ExperimentConfig):
        self.config = config
    
    def expand_symbols(self) -> List[str]:
        """å±•é–‹å¹£ç¨®åˆ—è¡¨"""
        
        if self.config.symbol_source == "explicit":
            return self.config.symbols
            
        elif self.config.symbol_source == "universe":
            universe_manager = UniverseManager()
            universe = universe_manager.load_latest_universe()
            
            symbols = universe.get_symbols_by_classification(
                self.config.universe_type, 
                top_n=self.config.universe_top_n
            )
            
            # æ‡‰ç”¨é¡å¤–ç¯©é¸
            if self.config.universe_filters:
                symbols = self._apply_universe_filters(symbols)
                
            return symbols
        
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„symbol_source: {self.config.symbol_source}")
    
    def expand_parameter_combinations(self) -> List[ParameterCombination]:
        """å±•é–‹åƒæ•¸çµ„åˆ"""
        
        # ç²å–æ‰€æœ‰åƒæ•¸åå’Œå€¼
        param_names = list(self.config.param_grid.keys())
        param_values = list(self.config.param_grid.values())
        
        # ç”Ÿæˆæ‰€æœ‰çµ„åˆ
        combinations = []
        for i, combo in enumerate(itertools.product(*param_values)):
            variable_params = dict(zip(param_names, combo))
            combined_params = {**self.config.strategy_params_base, **variable_params}
            
            combination = ParameterCombination(
                combination_id=f"combo_{i:04d}",
                base_params=self.config.strategy_params_base,
                variable_params=variable_params,
                combined_params=combined_params
            )
            combinations.append(combination)
        
        # é™åˆ¶çµ„åˆæ•¸é‡
        if len(combinations) > self.config.max_combinations:
            # éš¨æ©Ÿæ¡æ¨£æˆ–æ™ºèƒ½æ¡æ¨£
            combinations = self._sample_combinations(combinations)
        
        return combinations
    
    def expand_experiment_tasks(self) -> List[ExperimentTask]:
        """å±•é–‹æ‰€æœ‰å¯¦é©—ä»»å‹™"""
        
        symbols = self.expand_symbols()
        param_combinations = self.expand_parameter_combinations()
        
        tasks = []
        task_id = 0
        
        for symbol in symbols:
            for param_combo in param_combinations:
                task = ExperimentTask(
                    task_id=f"{self.config.name}_{task_id:06d}",
                    experiment_name=self.config.name,
                    symbol=symbol,
                    param_combination=param_combo,
                    config=self.config
                )
                tasks.append(task)
                task_id += 1
        
        return tasks
    
    def _sample_combinations(self, combinations: List[ParameterCombination]) -> List[ParameterCombination]:
        """æ™ºèƒ½æ¡æ¨£åƒæ•¸çµ„åˆ"""
        
        if self.config.optimization_metric == "random":
            # éš¨æ©Ÿæ¡æ¨£
            return random.sample(combinations, self.config.max_combinations)
        else:
            # ç¶²æ ¼æ¡æ¨£ï¼šç¢ºä¿åƒæ•¸ç©ºé–“å‡å‹»è¦†è“‹
            return self._grid_sample(combinations)
```

### 3. æ‰¹é‡åŸ·è¡Œå¼•æ“
```python
class ExperimentRunner:
    """å¯¦é©—æ‰¹é‡åŸ·è¡Œå¼•æ“"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.result_store = ExperimentResultStore()
        
    async def run_experiment(self, config: ExperimentConfig) -> ExperimentSummary:
        """åŸ·è¡Œå®Œæ•´å¯¦é©—"""
        
        print(f"ğŸš€ é–‹å§‹å¯¦é©—: {config.name}")
        start_time = datetime.now()
        
        # å±•é–‹å¯¦é©—ä»»å‹™
        expander = ParameterExpander(config)
        tasks = expander.expand_experiment_tasks()
        
        print(f"ğŸ“‹ ç¸½ä»»å‹™æ•¸: {len(tasks)}")
        print(f"ğŸ’° å¹£ç¨®æ•¸: {len(set(task.symbol for task in tasks))}")
        print(f"âš™ï¸ åƒæ•¸çµ„åˆæ•¸: {len(set(task.param_combination.combination_id for task in tasks))}")
        
        # ä¸¦è¡ŒåŸ·è¡Œä»»å‹™
        results = []
        failed_tasks = []
        
        async with aiofiles.TemporaryDirectory() as temp_dir:
            semaphore = asyncio.Semaphore(self.max_workers)
            
            async def run_single_task(task: ExperimentTask) -> Optional[ExperimentResult]:
                async with semaphore:
                    try:
                        return await self._execute_task(task)
                    except Exception as e:
                        failed_tasks.append((task, str(e)))
                        if config.fail_fast:
                            raise
                        return None
            
            # åŸ·è¡Œæ‰€æœ‰ä»»å‹™
            task_results = await asyncio.gather(*[
                run_single_task(task) for task in tasks
            ], return_exceptions=True)
            
            # ç¯©é¸æˆåŠŸçµæœ
            results = [r for r in task_results if isinstance(r, ExperimentResult)]
        
        # ä¿å­˜çµæœ
        experiment_summary = self._save_experiment_results(config, results, failed_tasks)
        
        end_time = datetime.now()
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"âœ… å¯¦é©—å®Œæˆ!")
        print(f"â±ï¸ åŸ·è¡Œæ™‚é–“: {execution_time:.1f} ç§’")
        print(f"âœ… æˆåŠŸä»»å‹™: {len(results)}")
        print(f"âŒ å¤±æ•—ä»»å‹™: {len(failed_tasks)}")
        
        return experiment_summary
    
    async def _execute_task(self, task: ExperimentTask) -> ExperimentResult:
        """åŸ·è¡Œå–®å€‹ä»»å‹™"""
        
        task.status = "running"
        task.start_time = datetime.now()
        
        try:
            # è¼‰å…¥ç­–ç•¥
            strategy_cls = get_strategy(task.config.strategy)
            
            # è¼‰å…¥æ•¸æ“š
            data = await self._load_task_data(task)
            
            # åŸ·è¡Œå›æ¸¬
            backtest_result = await self._run_backtest(
                strategy_cls, data, task.param_combination.combined_params, task.config
            )
            
            # è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™
            metrics = self._calculate_metrics(backtest_result)
            
            # ç”Ÿæˆçµæœ
            result = ExperimentResult(
                task_id=task.task_id,
                experiment_name=task.experiment_name,
                symbol=task.symbol,
                timeframe=task.config.timeframe,
                start_date=task.config.start_date,
                end_date=task.config.end_date,
                strategy=task.config.strategy,
                params_json=json.dumps(task.param_combination.combined_params),
                param_combination_id=task.param_combination.combination_id,
                **metrics,
                execution_time_seconds=(datetime.now() - task.start_time).total_seconds(),
                data_points=len(data),
                created_at=datetime.now()
            )
            
            task.status = "completed"
            task.end_time = datetime.now()
            
            return result
            
        except Exception as e:
            task.status = "failed"
            task.end_time = datetime.now()
            task.error_message = str(e)
            raise
    
    async def _load_task_data(self, task: ExperimentTask) -> Dict:
        """è¼‰å…¥ä»»å‹™æ•¸æ“š"""
        
        # ä½¿ç”¨ç¾æœ‰çš„æ•¸æ“šç®¡é“
        pipeline = get_data_pipeline()
        
        strategy_cls = get_strategy(task.config.strategy)
        strategy_instance = strategy_cls()
        
        # ç²å–ç­–ç•¥æ•¸æ“šéœ€æ±‚
        data_requirements = strategy_instance.get_data_requirements()
        
        # è¼‰å…¥æ•¸æ“š
        data = pipeline.load_strategy_data(
            strategy_instance,
            task.symbol,
            task.config.timeframe,
            start_date=task.config.start_date,
            end_date=task.config.end_date
        )
        
        return data
    
    def _calculate_metrics(self, backtest_result) -> Dict[str, float]:
        """è¨ˆç®—ç¸¾æ•ˆæŒ‡æ¨™"""
        
        from reports.performance_metrics import PerformanceCalculator
        
        calculator = PerformanceCalculator(backtest_result)
        
        return {
            'total_return': calculator.total_return(),
            'annualized_return': calculator.annualized_return(),
            'volatility': calculator.volatility(),
            'sharpe_ratio': calculator.sharpe_ratio(),
            'max_drawdown': calculator.max_drawdown(),
            'calmar_ratio': calculator.calmar_ratio(),
            'total_trades': calculator.total_trades(),
            'win_rate': calculator.win_rate(),
            'profit_factor': calculator.profit_factor(),
            'avg_win': calculator.avg_win(),
            'avg_loss': calculator.avg_loss(),
            'var_95': calculator.var_95(),
            'expected_shortfall': calculator.expected_shortfall()
        }
```

---

## ğŸ’¾ çµæœå­˜å„²ç³»çµ±

### å­˜å„²æ¶æ§‹
```python
class ExperimentResultStore:
    """å¯¦é©—çµæœå­˜å„²ç®¡ç†"""
    
    def __init__(self, storage_dir: str = "data/experiments/results"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
    
    def save_experiment_results(self, experiment_name: str, 
                              results: List[ExperimentResult]) -> str:
        """ä¿å­˜å¯¦é©—çµæœ"""
        
        # è½‰æ›ç‚ºDataFrame
        df = pd.DataFrame([asdict(result) for result in results])
        
        # ç”Ÿæˆæª”æ¡ˆå
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{experiment_name}_{timestamp}.parquet"
        filepath = self.storage_dir / filename
        
        # ä¿å­˜ç‚ºParquet
        df.to_parquet(filepath, compression='snappy')
        
        # æ›´æ–°ç´¢å¼•
        self._update_experiment_index(experiment_name, filename)
        
        return str(filepath)
    
    def load_experiment_results(self, experiment_name: str, 
                              run_id: str = None) -> pd.DataFrame:
        """è¼‰å…¥å¯¦é©—çµæœ"""
        
        if run_id:
            filepath = self.storage_dir / f"{experiment_name}_{run_id}.parquet"
        else:
            # è¼‰å…¥æœ€æ–°çµæœ
            filepath = self._get_latest_result_file(experiment_name)
        
        return pd.read_parquet(filepath)
    
    def query_results(self, filters: Dict = None, 
                     sort_by: str = None, limit: int = None) -> pd.DataFrame:
        """æŸ¥è©¢å¯¦é©—çµæœ"""
        
        # è¼‰å…¥æ‰€æœ‰çµæœæ–‡ä»¶
        all_results = []
        
        for result_file in self.storage_dir.glob("*.parquet"):
            df = pd.read_parquet(result_file)
            all_results.append(df)
        
        if not all_results:
            return pd.DataFrame()
        
        # åˆä½µæ‰€æœ‰çµæœ
        combined_df = pd.concat(all_results, ignore_index=True)
        
        # æ‡‰ç”¨ç¯©é¸æ¢ä»¶
        if filters:
            for column, condition in filters.items():
                if isinstance(condition, dict):
                    # ç¯„åœæ¢ä»¶ {'min': 0.1, 'max': 0.5}
                    if 'min' in condition:
                        combined_df = combined_df[combined_df[column] >= condition['min']]
                    if 'max' in condition:
                        combined_df = combined_df[combined_df[column] <= condition['max']]
                else:
                    # ç­‰å€¼æ¢ä»¶
                    combined_df = combined_df[combined_df[column] == condition]
        
        # æ’åº
        if sort_by:
            ascending = not sort_by.startswith('-')
            sort_column = sort_by.lstrip('-')
            combined_df = combined_df.sort_values(sort_column, ascending=ascending)
        
        # é™åˆ¶çµæœæ•¸é‡
        if limit:
            combined_df = combined_df.head(limit)
        
        return combined_df
```

---

## ğŸ“Š çµæœåˆ†æå™¨

### åˆ†æåŠŸèƒ½
```python
class ExperimentAnalyzer:
    """å¯¦é©—çµæœåˆ†æå™¨"""
    
    def __init__(self, result_store: ExperimentResultStore):
        self.result_store = result_store
    
    def find_best_parameters(self, experiment_name: str, 
                           metric: str = 'sharpe_ratio',
                           top_n: int = 10) -> pd.DataFrame:
        """æ‰¾å‡ºæœ€ä½³åƒæ•¸çµ„åˆ"""
        
        results = self.result_store.load_experiment_results(experiment_name)
        
        # æŒ‰æŒ‡æ¨™æ’åº
        ascending = metric in ['max_drawdown', 'volatility']  # é€™äº›æŒ‡æ¨™è¶Šå°è¶Šå¥½
        best_results = results.nlargest(top_n, metric) if not ascending else results.nsmallest(top_n, metric)
        
        return best_results[['symbol', 'params_json', metric, 'total_return', 'win_rate']]
    
    def analyze_parameter_sensitivity(self, experiment_name: str, 
                                    target_metric: str = 'sharpe_ratio') -> Dict:
        """åˆ†æåƒæ•¸æ•æ„Ÿæ€§"""
        
        results = self.result_store.load_experiment_results(experiment_name)
        
        # è§£æåƒæ•¸JSON
        results['params'] = results['params_json'].apply(json.loads)
        
        sensitivity_analysis = {}
        
        # åˆ†ææ¯å€‹åƒæ•¸çš„å½±éŸ¿
        for result in results.itertuples():
            params = result.params
            metric_value = getattr(result, target_metric)
            
            for param_name, param_value in params.items():
                if param_name not in sensitivity_analysis:
                    sensitivity_analysis[param_name] = []
                
                sensitivity_analysis[param_name].append({
                    'value': param_value,
                    'metric': metric_value
                })
        
        # è¨ˆç®—ç›¸é—œæ€§
        correlations = {}
        for param_name, data in sensitivity_analysis.items():
            df = pd.DataFrame(data)
            if df['value'].dtype in ['int64', 'float64']:
                correlation = df['value'].corr(df['metric'])
                correlations[param_name] = correlation
        
        return {
            'correlations': correlations,
            'sensitivity_data': sensitivity_analysis
        }
    
    def compare_strategies(self, experiment_names: List[str], 
                          metric: str = 'sharpe_ratio') -> pd.DataFrame:
        """æ¯”è¼ƒä¸åŒç­–ç•¥è¡¨ç¾"""
        
        comparison_data = []
        
        for exp_name in experiment_names:
            results = self.result_store.load_experiment_results(exp_name)
            
            summary = {
                'experiment': exp_name,
                'strategy': results['strategy'].iloc[0],
                'total_combinations': len(results),
                f'best_{metric}': results[metric].max(),
                f'avg_{metric}': results[metric].mean(),
                f'std_{metric}': results[metric].std(),
                'success_rate': (results[metric] > 0).sum() / len(results) * 100
            }
            
            comparison_data.append(summary)
        
        return pd.DataFrame(comparison_data)
```

---

## ğŸ–¥ï¸ CLIæ¥å£è¨­è¨ˆ

### å‘½ä»¤çµæ§‹
```python
@click.group(name='experiment')
def experiment_commands():
    """ç­–ç•¥å¯¦é©—å®¤å‘½ä»¤"""
    pass

@experiment_commands.command()
@click.argument('config_file', type=click.Path(exists=True))
@click.option('--dry-run', is_flag=True, help='åƒ…é©—è­‰é…ç½®ï¼Œä¸åŸ·è¡Œ')
@click.option('--parallel', default=4, help='ä¸¦è¡ŒåŸ·è¡Œæ•¸é‡')
def run(config_file, dry_run, parallel):
    """åŸ·è¡Œç­–ç•¥å¯¦é©—
    
    Examples:
        superdog experiment run experiments/kawamoku_optimization.yml
        superdog experiment run config.yml --dry-run
    """
    
@experiment_commands.command()
@click.argument('experiment_name')
@click.option('--metric', default='sharpe_ratio', help='æ’åºæŒ‡æ¨™')
@click.option('--top', default=10, help='é¡¯ç¤ºå‰Nå€‹çµæœ')
@click.option('--format', type=click.Choice(['table', 'json']), default='table')
def best(experiment_name, metric, top, format):
    """é¡¯ç¤ºæœ€ä½³åƒæ•¸çµ„åˆ
    
    Examples:
        superdog experiment best kawamoku_opt --metric sharpe_ratio --top 5
        superdog experiment best myexp --format json
    """

@experiment_commands.command()  
@click.option('--experiment', help='å¯¦é©—åç¨±ç¯©é¸')
@click.option('--strategy', help='ç­–ç•¥åç¨±ç¯©é¸')
@click.option('--symbol', help='äº¤æ˜“å°ç¯©é¸(æ”¯æ´é€šé…ç¬¦)')
@click.option('--metric', help='æŒ‡æ¨™ç¯©é¸ (æ ¼å¼: metric:min:max)')
def filter(experiment, strategy, symbol, metric):
    """ç¯©é¸å¯¦é©—çµæœ
    
    Examples:
        superdog experiment filter --experiment kawamoku* --symbol BTC*
        superdog experiment filter --metric sharpe_ratio:0.5:2.0
    """

@experiment_commands.command()
@click.argument('experiment_name')
@click.option('--detailed', is_flag=True, help='é¡¯ç¤ºè©³ç´°çµ±è¨ˆ')
def show(experiment_name, detailed):
    """é¡¯ç¤ºå¯¦é©—æ‘˜è¦
    
    Examples:
        superdog experiment show kawamoku_optimization
        superdog experiment show myexp --detailed
    """
```

---

## âš¡ æ€§èƒ½å„ªåŒ–

### ä¸¦è¡ŒåŸ·è¡Œå„ªåŒ–
```python
class OptimizedExperimentRunner(ExperimentRunner):
    """å„ªåŒ–çš„å¯¦é©—åŸ·è¡Œå™¨"""
    
    def __init__(self, max_workers: int = 8):
        super().__init__(max_workers)
        self.data_cache = LRUCache(maxsize=100)  # æ•¸æ“šå¿«å–
        self.strategy_cache = {}                 # ç­–ç•¥å¯¦ä¾‹å¿«å–
    
    async def _load_task_data_optimized(self, task: ExperimentTask) -> Dict:
        """å„ªåŒ–çš„æ•¸æ“šè¼‰å…¥"""
        
        cache_key = f"{task.symbol}_{task.config.timeframe}_{task.config.start_date}_{task.config.end_date}"
        
        if cache_key in self.data_cache:
            return self.data_cache[cache_key]
        
        data = await self._load_task_data(task)
        self.data_cache[cache_key] = data
        
        return data
    
    def _batch_similar_tasks(self, tasks: List[ExperimentTask]) -> List[List[ExperimentTask]]:
        """å°‡ç›¸ä¼¼ä»»å‹™åˆ†æ‰¹è™•ç†"""
        
        # æŒ‰(symbol, timeframe)åˆ†çµ„
        grouped_tasks = defaultdict(list)
        
        for task in tasks:
            key = (task.symbol, task.config.timeframe)
            grouped_tasks[key].append(task)
        
        # æ¯æ‰¹æœ€å¤šè™•ç†åŒä¸€è³‡ç”¢çš„10å€‹åƒæ•¸çµ„åˆ
        batches = []
        for group_tasks in grouped_tasks.values():
            for i in range(0, len(group_tasks), 10):
                batches.append(group_tasks[i:i+10])
        
        return batches
```

### è¨˜æ†¶é«”ç®¡ç†
```python
class MemoryEfficientRunner:
    """è¨˜æ†¶é«”é«˜æ•ˆçš„åŸ·è¡Œå™¨"""
    
    def __init__(self, max_memory_gb: float = 4.0):
        self.max_memory_gb = max_memory_gb
        self.current_memory_usage = 0
    
    async def run_with_memory_limit(self, tasks: List[ExperimentTask]):
        """åœ¨è¨˜æ†¶é«”é™åˆ¶ä¸‹åŸ·è¡Œä»»å‹™"""
        
        task_queue = deque(tasks)
        running_tasks = []
        
        while task_queue or running_tasks:
            # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡
            current_memory = self._get_memory_usage()
            
            if current_memory < self.max_memory_gb and task_queue:
                # å•Ÿå‹•æ–°ä»»å‹™
                task = task_queue.popleft()
                task_coroutine = self._run_memory_tracked_task(task)
                running_tasks.append(asyncio.create_task(task_coroutine))
            
            # ç­‰å¾…ä»»ä¸€ä»»å‹™å®Œæˆ
            if running_tasks:
                done, pending = await asyncio.wait(
                    running_tasks, return_when=asyncio.FIRST_COMPLETED
                )
                
                # è™•ç†å®Œæˆçš„ä»»å‹™
                for task in done:
                    result = await task
                    self._save_result(result)
                
                running_tasks = list(pending)
            
            await asyncio.sleep(0.1)  # çŸ­æš«ä¼‘æ¯
```

é€™å€‹æŠ€è¡“è¦æ ¼æä¾›äº†ç­–ç•¥å¯¦é©—å®¤çš„å®Œæ•´è¨­è¨ˆè—åœ–ï¼Œç¢ºä¿v0.6èƒ½å¤ æä¾›å¼·å¤§çš„æ‰¹é‡å¯¦é©—å’Œåƒæ•¸å„ªåŒ–åŠŸèƒ½ã€‚
