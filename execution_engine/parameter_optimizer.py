"""
Parameter Optimizer v0.6

åƒæ•¸å„ªåŒ–å™¨ - æ™ºèƒ½æœç´¢ã€è²è‘‰æ–¯å„ªåŒ–ã€æ—©åœç­–ç•¥

æ ¸å¿ƒåŠŸèƒ½:
- ç¶²æ ¼æœç´¢ï¼ˆGrid Searchï¼‰
- éš¨æ©Ÿæœç´¢ï¼ˆRandom Searchï¼‰
- è²è‘‰æ–¯å„ªåŒ–ï¼ˆBayesian Optimizationï¼‰
- æ—©åœç­–ç•¥ï¼ˆEarly Stoppingï¼‰
- åƒæ•¸é‡è¦æ€§åˆ†æ

Version: v0.6 Phase 2
Design Reference: docs/specs/v0.6/superdog_v06_strategy_lab_spec.md
"""

import json
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple

import numpy as np

from .experiment_runner import ExperimentRunner, ParameterExpander
from .experiments import (
    ExperimentConfig,
    ExperimentResult,
    ExperimentRun,
    ExperimentStatus,
    ParameterRange,
)


class OptimizationMode(Enum):
    """å„ªåŒ–æ¨¡å¼"""

    GRID = "grid"  # ç¶²æ ¼æœç´¢
    RANDOM = "random"  # éš¨æ©Ÿæœç´¢
    BAYESIAN = "bayesian"  # è²è‘‰æ–¯å„ªåŒ–
    GENETIC = "genetic"  # éºå‚³ç®—æ³•ï¼ˆæœªå¯¦ç¾ï¼‰


@dataclass
class OptimizationConfig:
    """å„ªåŒ–é…ç½®"""

    mode: OptimizationMode = OptimizationMode.GRID
    metric: str = "sharpe_ratio"  # å„ªåŒ–ç›®æ¨™æŒ‡æ¨™
    maximize: bool = True  # True=æœ€å¤§åŒ–ï¼ŒFalse=æœ€å°åŒ–

    # æ—©åœé…ç½®
    early_stopping: bool = False  # æ˜¯å¦å•Ÿç”¨æ—©åœ
    patience: int = 10  # å®¹å¿è¼ªæ•¸
    min_improvement: float = 0.01  # æœ€å°æ”¹é€²å¹…åº¦

    # è²è‘‰æ–¯å„ªåŒ–é…ç½®
    n_initial_points: int = 10  # åˆå§‹éš¨æ©Ÿé»æ•¸
    acquisition_function: str = "EI"  # Expected Improvement

    # ä¸¦è¡Œé…ç½®
    max_workers: int = 4


class ParameterOptimizer:
    """åƒæ•¸å„ªåŒ–å™¨

    æä¾›å¤šç¨®å„ªåŒ–ç­–ç•¥ï¼Œæ‰¾å‡ºæœ€ä½³åƒæ•¸çµ„åˆ

    Example:
        >>> optimizer = ParameterOptimizer(
        ...     config=experiment_config,
        ...     backtest_func=my_backtest,
        ...     opt_config=OptimizationConfig(mode=OptimizationMode.BAYESIAN)
        ... )
        >>> result = optimizer.optimize()
        >>> print(f"æœ€ä½³åƒæ•¸: {result.best_run.parameters}")
    """

    def __init__(
        self,
        config: ExperimentConfig,
        backtest_func: Callable,
        opt_config: Optional[OptimizationConfig] = None,
    ):
        """åˆå§‹åŒ–

        Args:
            config: å¯¦é©—é…ç½®
            backtest_func: å›æ¸¬å‡½æ•¸
            opt_config: å„ªåŒ–é…ç½®ï¼ˆé»˜èªç‚ºç¶²æ ¼æœç´¢ï¼‰
        """
        self.config = config
        self.backtest_func = backtest_func
        self.opt_config = opt_config or OptimizationConfig()

        self.runner = ExperimentRunner(max_workers=self.opt_config.max_workers)

        # å„ªåŒ–ç‹€æ…‹
        self.best_score = float("-inf") if self.opt_config.maximize else float("inf")
        self.no_improvement_count = 0
        self.iteration = 0

    def optimize(self) -> ExperimentResult:
        """åŸ·è¡Œå„ªåŒ–

        Returns:
            ExperimentResult: å„ªåŒ–çµæœ
        """
        print(f"ğŸ¯ é–‹å§‹åƒæ•¸å„ªåŒ–: {self.opt_config.mode.value}")
        print(f"ğŸ“Š å„ªåŒ–æŒ‡æ¨™: {self.opt_config.metric} ({'æœ€å¤§åŒ–' if self.opt_config.maximize else 'æœ€å°åŒ–'})")

        if self.opt_config.mode == OptimizationMode.GRID:
            return self._grid_search()
        elif self.opt_config.mode == OptimizationMode.RANDOM:
            return self._random_search()
        elif self.opt_config.mode == OptimizationMode.BAYESIAN:
            return self._bayesian_optimization()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„å„ªåŒ–æ¨¡å¼: {self.opt_config.mode}")

    def _grid_search(self) -> ExperimentResult:
        """ç¶²æ ¼æœç´¢

        éæ­·æ‰€æœ‰åƒæ•¸çµ„åˆ

        Returns:
            ExperimentResult: å„ªåŒ–çµæœ
        """
        # ä½¿ç”¨æ¨™æº– ExperimentRunner
        result = self.runner.run_experiment(self.config, self.backtest_func)

        # æ—©åœæª¢æŸ¥ï¼ˆä¸é©ç”¨æ–¼ç¶²æ ¼æœç´¢ï¼Œå› ç‚ºæ˜¯ä¸€æ¬¡æ€§åŸ·è¡Œï¼‰
        return result

    def _random_search(self) -> ExperimentResult:
        """éš¨æ©Ÿæœç´¢

        éš¨æ©Ÿæ¡æ¨£åƒæ•¸ç©ºé–“ï¼Œæ”¯æ´æ—©åœ

        Returns:
            ExperimentResult: å„ªåŒ–çµæœ
        """
        # è¨­ç½®ç‚ºéš¨æ©Ÿæ¨¡å¼
        from .experiments import ParameterExpansionMode

        original_mode = self.config.expansion_mode
        self.config.expansion_mode = ParameterExpansionMode.RANDOM

        if self.opt_config.early_stopping:
            result = self._random_search_with_early_stopping()
        else:
            result = self.runner.run_experiment(self.config, self.backtest_func)

        # æ¢å¾©åŸæ¨¡å¼
        self.config.expansion_mode = original_mode

        return result

    def _random_search_with_early_stopping(self) -> ExperimentResult:
        """å¸¶æ—©åœçš„éš¨æ©Ÿæœç´¢

        Returns:
            ExperimentResult: å„ªåŒ–çµæœ
        """
        expander = ParameterExpander(self.config)
        all_tasks = expander.expand_tasks()

        # åˆ†æ‰¹åŸ·è¡Œ
        batch_size = 20
        all_runs = []
        experiment_id = self.config.get_experiment_id()

        for i in range(0, len(all_tasks), batch_size):
            batch_tasks = all_tasks[i : i + batch_size]
            print(f"\nğŸ“¦ æ‰¹æ¬¡ {i//batch_size + 1}/{(len(all_tasks)-1)//batch_size + 1}")

            # åŸ·è¡Œæ‰¹æ¬¡
            batch_runs = self._execute_batch(batch_tasks, experiment_id)
            all_runs.extend(batch_runs)

            # æª¢æŸ¥æ—©åœ
            if self._should_stop_early(all_runs):
                print(f"â¹ï¸  æ—©åœè§¸ç™¼ï¼Œå·²åŸ·è¡Œ {len(all_runs)}/{len(all_tasks)} å€‹ä»»å‹™")
                break

        # å‰µå»ºçµæœ
        return self._create_result(experiment_id, all_runs)

    def _bayesian_optimization(self) -> ExperimentResult:
        """è²è‘‰æ–¯å„ªåŒ–

        ä½¿ç”¨é«˜æ–¯éç¨‹é€²è¡Œæ™ºèƒ½æœç´¢

        Returns:
            ExperimentResult: å„ªåŒ–çµæœ
        """
        print("âš ï¸  è²è‘‰æ–¯å„ªåŒ–éœ€è¦å®‰è£ scikit-optimize")
        print("    ä½¿ç”¨éš¨æ©Ÿæœç´¢æ›¿ä»£...")

        try:
            from skopt import gp_minimize
            from skopt.space import Categorical, Integer, Real
            from skopt.utils import use_named_args
        except ImportError:
            print("âŒ scikit-optimize æœªå®‰è£ï¼Œå›é€€åˆ°éš¨æ©Ÿæœç´¢")
            return self._random_search()

        # å®šç¾©æœç´¢ç©ºé–“
        search_space = self._create_search_space()
        param_names = list(self.config.parameters.keys())

        # å®šç¾©ç›®æ¨™å‡½æ•¸
        @use_named_args(search_space)
        def objective(**params):
            # åŸ·è¡Œå›æ¸¬
            symbol = self.config.symbols[0]  # è²è‘‰æ–¯å„ªåŒ–æ™‚é€šå¸¸ç”¨å–®å€‹symbol
            metrics = self.backtest_func(symbol, self.config.timeframe, params, self.config)

            # è¿”å›è² æ•¸ï¼ˆå› ç‚º gp_minimize æ˜¯æœ€å°åŒ–ï¼‰
            score = metrics.get(self.opt_config.metric, 0)
            return -score if self.opt_config.maximize else score

        # åŸ·è¡Œå„ªåŒ–
        print(f"ğŸ” é–‹å§‹è²è‘‰æ–¯æœç´¢...")
        n_calls = self.config.max_combinations or 100

        result_bo = gp_minimize(
            objective,
            search_space,
            n_calls=n_calls,
            n_initial_points=self.opt_config.n_initial_points,
            acq_func=self.opt_config.acquisition_function,
            random_state=42,
            verbose=True,
        )

        # è½‰æ›ç‚º ExperimentResult
        return self._convert_bayesian_result(result_bo, param_names)

    def _create_search_space(self) -> List:
        """å‰µå»º scikit-optimize æœç´¢ç©ºé–“

        Returns:
            List: æœç´¢ç©ºé–“å®šç¾©
        """
        from skopt.space import Integer, Real

        space = []
        for name, param_range in self.config.parameters.items():
            if param_range.values is not None:
                # é›¢æ•£å€¼
                values = param_range.values
                if all(isinstance(v, int) for v in values):
                    space.append(Integer(min(values), max(values), name=name))
                else:
                    space.append(Real(min(values), max(values), name=name))
            else:
                # é€£çºŒç¯„åœ
                if param_range.log_scale:
                    space.append(
                        Real(param_range.start, param_range.stop, prior="log-uniform", name=name)
                    )
                else:
                    space.append(Real(param_range.start, param_range.stop, name=name))

        return space

    def _execute_batch(self, tasks: List[Dict], experiment_id: str) -> List[ExperimentRun]:
        """åŸ·è¡Œä¸€æ‰¹ä»»å‹™

        Args:
            tasks: ä»»å‹™åˆ—è¡¨
            experiment_id: å¯¦é©—ID

        Returns:
            List[ExperimentRun]: é‹è¡Œè¨˜éŒ„
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed

        runs = []
        with ThreadPoolExecutor(max_workers=self.opt_config.max_workers) as executor:
            futures = []
            for i, task in enumerate(tasks):
                run_id = f"{experiment_id}_run_{self.iteration:06d}"
                self.iteration += 1

                future = executor.submit(
                    self.runner._execute_single_run,
                    run_id=run_id,
                    experiment_id=experiment_id,
                    symbol=task["symbol"],
                    parameters=task["parameters"],
                    config=self.config,
                    backtest_func=self.backtest_func,
                )
                futures.append(future)

            for future in as_completed(futures):
                run = future.result()
                runs.append(run)

        return runs

    def _should_stop_early(self, runs: List[ExperimentRun]) -> bool:
        """æª¢æŸ¥æ˜¯å¦æ‡‰è©²æ—©åœ

        Args:
            runs: æ‰€æœ‰é‹è¡Œè¨˜éŒ„

        Returns:
            bool: True=æ‡‰è©²åœæ­¢
        """
        if not self.opt_config.early_stopping:
            return False

        # æ‰¾å‡ºå®Œæˆçš„é‹è¡Œ
        completed = [r for r in runs if r.status == ExperimentStatus.COMPLETED]
        if len(completed) < 2:
            return False

        # ç²å–ç•¶å‰æ‰¹æ¬¡çš„æœ€ä½³åˆ†æ•¸
        current_best = self._get_best_score(completed)

        # æª¢æŸ¥æ˜¯å¦æœ‰æ”¹é€²
        if self.opt_config.maximize:
            improvement = current_best - self.best_score
        else:
            improvement = self.best_score - current_best

        if improvement > self.opt_config.min_improvement:
            # æœ‰æ”¹é€²ï¼Œé‡ç½®è¨ˆæ•¸
            self.best_score = current_best
            self.no_improvement_count = 0
            print(f"âœ¨ æ‰¾åˆ°æ›´å¥½çš„åƒæ•¸ï¼Œ{self.opt_config.metric} = {current_best:.4f}")
        else:
            # ç„¡æ”¹é€²ï¼Œå¢åŠ è¨ˆæ•¸
            self.no_improvement_count += 1

        # æª¢æŸ¥æ˜¯å¦é”åˆ°å®¹å¿åº¦
        return self.no_improvement_count >= self.opt_config.patience

    def _get_best_score(self, runs: List[ExperimentRun]) -> float:
        """ç²å–æœ€ä½³åˆ†æ•¸

        Args:
            runs: é‹è¡Œè¨˜éŒ„

        Returns:
            float: æœ€ä½³åˆ†æ•¸
        """
        scores = []
        for run in runs:
            score = getattr(run, self.opt_config.metric, None)
            if score is None:
                score = run.metrics.get(self.opt_config.metric)
            if score is not None:
                scores.append(score)

        if not scores:
            return float("-inf") if self.opt_config.maximize else float("inf")

        return max(scores) if self.opt_config.maximize else min(scores)

    def _create_result(self, experiment_id: str, runs: List[ExperimentRun]) -> ExperimentResult:
        """å‰µå»ºå¯¦é©—çµæœ

        Args:
            experiment_id: å¯¦é©—ID
            runs: é‹è¡Œè¨˜éŒ„

        Returns:
            ExperimentResult: å¯¦é©—çµæœ
        """
        completed = [r for r in runs if r.status == ExperimentStatus.COMPLETED]
        failed = [r for r in runs if r.status == ExperimentStatus.FAILED]

        result = ExperimentResult(
            experiment_id=experiment_id,
            config=self.config,
            runs=runs,
            total_runs=len(runs),
            completed_runs=len(completed),
            failed_runs=len(failed),
            best_metric=self.opt_config.metric,
        )

        result.best_run = result.get_best_run(
            metric=self.opt_config.metric, ascending=not self.opt_config.maximize
        )

        return result

    def _convert_bayesian_result(self, result_bo: Any, param_names: List[str]) -> ExperimentResult:
        """è½‰æ›è²è‘‰æ–¯å„ªåŒ–çµæœ

        Args:
            result_bo: scikit-optimize çµæœ
            param_names: åƒæ•¸åç¨±åˆ—è¡¨

        Returns:
            ExperimentResult: å¯¦é©—çµæœ
        """
        # å‰µå»ºé‹è¡Œè¨˜éŒ„
        runs = []
        for i, (x, y) in enumerate(zip(result_bo.x_iters, result_bo.func_vals)):
            params = dict(zip(param_names, x))

            # åè½‰åˆ†æ•¸ï¼ˆä¹‹å‰ç‚ºäº†æœ€å°åŒ–å–äº†è² æ•¸ï¼‰
            score = -y if self.opt_config.maximize else y

            run = ExperimentRun(
                experiment_id=self.config.get_experiment_id(),
                run_id=f"bayesian_run_{i:04d}",
                symbol=self.config.symbols[0],
                parameters=params,
                status=ExperimentStatus.COMPLETED,
            )

            # è¨­ç½®æŒ‡æ¨™
            setattr(run, self.opt_config.metric, score)

            runs.append(run)

        # å‰µå»ºçµæœ
        return self._create_result(self.config.get_experiment_id(), runs)

    def analyze_parameter_importance(self, result: ExperimentResult) -> Dict[str, float]:
        """åˆ†æåƒæ•¸é‡è¦æ€§

        ä½¿ç”¨æ–¹å·®åˆ†æä¾†è©•ä¼°æ¯å€‹åƒæ•¸å°çµæœçš„å½±éŸ¿

        Args:
            result: å¯¦é©—çµæœ

        Returns:
            Dict[str, float]: åƒæ•¸é‡è¦æ€§åˆ†æ•¸ï¼ˆ0-1ï¼‰
        """
        import pandas as pd

        # æå–æ•¸æ“š
        data = []
        for run in result.runs:
            if run.status == ExperimentStatus.COMPLETED:
                row = run.parameters.copy()
                row["_metric"] = getattr(run, self.opt_config.metric, None)
                if row["_metric"] is not None:
                    data.append(row)

        if not data:
            return {}

        df = pd.DataFrame(data)

        # è¨ˆç®—æ¯å€‹åƒæ•¸çš„æ–¹å·®è²¢ç»
        importance = {}
        total_variance = df["_metric"].var()

        for param in [c for c in df.columns if c != "_metric"]:
            # è¨ˆç®—åˆ†çµ„æ–¹å·®
            grouped = df.groupby(param)["_metric"].var()
            param_variance = grouped.mean()

            # æ–¹å·®æ¯”ä¾‹
            importance[param] = param_variance / total_variance if total_variance > 0 else 0

        # æ­¸ä¸€åŒ–
        total = sum(importance.values())
        if total > 0:
            importance = {k: v / total for k, v in importance.items()}

        return importance


# ===== ä¾¿æ·å‡½æ•¸ =====


def optimize_parameters(
    config: ExperimentConfig,
    backtest_func: Callable,
    mode: str = "grid",
    metric: str = "sharpe_ratio",
    **kwargs,
) -> ExperimentResult:
    """å„ªåŒ–åƒæ•¸çš„ä¾¿æ·å‡½æ•¸

    Args:
        config: å¯¦é©—é…ç½®
        backtest_func: å›æ¸¬å‡½æ•¸
        mode: å„ªåŒ–æ¨¡å¼ï¼ˆgrid/random/bayesianï¼‰
        metric: å„ªåŒ–æŒ‡æ¨™
        **kwargs: å…¶ä»–å„ªåŒ–é…ç½®

    Returns:
        ExperimentResult: å„ªåŒ–çµæœ

    Example:
        >>> result = optimize_parameters(
        ...     config=my_config,
        ...     backtest_func=my_backtest,
        ...     mode="bayesian",
        ...     metric="sharpe_ratio"
        ... )
    """
    opt_config = OptimizationConfig(mode=OptimizationMode(mode), metric=metric, **kwargs)

    optimizer = ParameterOptimizer(config, backtest_func, opt_config)
    return optimizer.optimize()
